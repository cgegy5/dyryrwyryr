\documentclass[main.tex]{subfiles}
 
\begin{document}
%\section{*Abstract Integration}
%\towrite{Lebesgue-Stieltjes integral}
%\todo{See Rynne or Garling or Analyse de Hilbert et de Fourier course notes}
%\section{*Probability Measure}
%\todo{See Pestman or MIT course notes or...}
%\towrite{Should cover: p. measure, cont. distr., stochastic vectors, E, moment gen?, CLT, multivar norm dist}
%\section{Large Deviation Theory}
%\todo{See Hugo or MIT course notes or ...}
%\towrite{Should cover: application to multivariate normal dist}

\section{Multivariate Gaussian distribution}
\emph{This presentation of the multivariate Gaussian distribution is heavily based on Chapter VIII of \emph{Mathematical Statistics} by \cite{Pestman1998}, which provides proofs to all theorems listed below.}

One distribution that will be particularly useful in our analysis is the \emph{multivariate Gaussian distribution}, which generalises the (one-dimensional) normal distribution. In the simplest case, we have a stochastic vector $\mat{E}=(\mel{E}_1, \dots, \mel{E}_p)$, which is the combination of $p$ Gaussian distributed, \emph{independent} scalar variables. In general, however, we wish to study stochastic vectors produced by applying a \emph{linear transformation} $\mat{L} \in \mathcal{L}(\mathbb{R}^p, \mathbb{R}^q)$ to $\mat{E}$. In this case, the coordinate variables $(\mel{LE})_1,\dots,(\mel{LE})_q$ are not always independent! For example, the map $(\mel{E}_1, \mel{E}_2) \mapsto (\mel{E}_1, \mel{E}_1 + \mel{E}_2)$ transforms two independent normals into two dependent ones.
%In particular, we answer the following question: ``For $i,j \in \range{q}$ and $x \in \mathbb{R}$, what is the distribution of $(\mel{LE})_i$, \emph{given that} $(\mel{LE})_j \geq x$?''
\subsection{Normal and Gaussian}
Although they are often used interchangeably, we make a clear distinction between a \emph{normal} distribution and a \emph{Gaussian} distribution. The former should be familiar:

\begin{definition}
A scalar variable $E$ is said to be \emph{normally distributed} with parameters $\mu$ and $\sigma$ if
\begin{equation}
x \mapsto \frac{1}{\sigma \sqrt{2\pi}} \exp\left[\frac{-(x-\mu)^2}{2\sigma^2}\right]
\end{equation}
is the probability density of $E$.
\end{definition}

\begin{definition}
A scalar variable $E$ is said to be \emph{Gaussian distributed} if it is either normally distributed or constant. 
\end{definition}

One could interpret a constant $E$ with value $\mu$ as a normally distributed variable with mean $\mu$ and `standard deviation $0$'. A linear combination of normally distributed \emph{scalar} variables is also normally distributed, and the same is true for Gaussian distributed scalars.

\begin{definition}
A stochastic vector $\mat{E}=(\mel{E}_1, \dots, \mel{E}_p)$ is \emph{elementary normally distributed}\index{distribution!elementary normal} if the scalar variables $\mel{E}_i$ are independent and normally distributed ($i \in \range{p}$).
$\mat{E}$ is \emph{elementary Gaussian distributed}\index{distribution!elementary Gaussian} if the scalar variables $\mel{E}_i$ are independent and Gaussian distributed.
\end{definition}

\begin{definition}
A stochastic vector $\mat{X}=(\mel{X}_1, \dots, \mel{X}_p)$ is \emph{normally distributed}\index{distribution!normal} if there exists an orthogonal operator $\mat{Q}$ such that $\mat{QX}$ is elementary normally distributed.
$\mat{X}$ is \emph{Gaussian distributed}\index{distribution!Gaussian} if there exists an orthogonal operator $\mat{Q}$ such that $\mat{QX}$ is elementary Gaussian distributed.
\end{definition}
%
%\towrite{Plaatjes vullen gaatjes}
%
We state, without proof, the following properties of Gaussian distributed vectors:
\begin{proposition}\label{thm:normaliffinvertible}
The distribution of a Gaussian distributed vector $\mat{X}$ is uniquely determined by its expectation $\mat{\mu} =\EXP\left[\mat{X}\right]$ and covariance matrix $\mat{\Sigma}=\COVMAT(\mat{X})$, and we write $\mat{X} \sim \gaussdistr(\mat{\mu}, \mat{\Sigma})$. $\mat{X}$ is normally distributed if and only if $\mat{\Sigma}$ is invertible.
\end{proposition}

Translating or applying a linear map to a Gaussian distribution results in a new Gaussian distribution. Note that this can be \emph{any} linear map, not necessarily a bijective, orthogonal one.

\begin{theorem}\label{thm:linearmapofgaussian}
Suppose $\mat{X}$ is a $\gaussdistr(\mat{\mu}, \mat{\Sigma})$-distributed $p$-vector. 
\begin{align}
\intertext{For any $\mat{a} \in \mathbb{R}^p$:}
\mat{X}+\mat{a} \, &\sim \, \gaussdistr(\mat{\mu}+\mat{a}, \mat{\Sigma}), \\
\intertext{and for any linear map $\mat{L} \in \mathcal{L}(\mathbb{R}^p, \mathbb{R}^q)$:}
\mat{L}\mat{X} \, &\sim \, \gaussdistr(\mat{L}\mat{\mu}, \mat{L}\mat{\Sigma}\mat{L}^*).
\end{align}
\end{theorem}
\begin{remark}
If $\mat{X}$ is normally distributed, and $\mat{L} \in \mathcal{L}(\mathbb{R}^p, \mathbb{R}^q)$, then $\mat{L}\mat{X}$ is Gaussian distributed, but not necessarily normally distributed! A trivial example is the zero map: any normally distributed scalar is mapped to a constant (0), which does not exhibit a probability density function.
\end{remark}
\begin{corollary}\label{cor:innerproductisgaussian}
For any $\mat{b} \in \mathbb{R}^p$, the mapping
\[
\left\langle\mat{b}, \,\cdot\,\right\rangle : \mathbb{R}^p \rightarrow \mathbb{R} : \mat{x} \mapsto \left\langle\mat{b}, \mat{x}\right\rangle
\]
is \emph{linear}, and therefore the scalar variable $\left\langle\mat{b}, \mat{X}\right\rangle$ is Gaussian distributed.
\end{corollary}
In particular, when applying Corollary \ref{cor:innerproductisgaussian} to each element of the standard basis $(\mat{e}_1, \dots, \mat{e}_p)$ of $\mathbb{R}^p$, we find that each of the \emph{coordinates} of $\mat{X}$ is Gaussian distributed:
\begin{proposition}\label{prop:gaussianmarginaldistr}
Suppose $\mat{X}=(\mel{X}_1,\dots,\mel{X}_p)$ is a $\gaussdistr(\mat{\mu}, \mat{\Sigma})$-distributed $p$-vector. Then each for each $i \in \range{p}$, the marginal distribution is given by:
\[
\mel{X}_i \, \sim \, \gaussdistr(\mel{\mu}_i, \mel{\Sigma}_{ii}).
\]
\end{proposition}


For normally distributed vectors, a probability density function exists:
\begin{theorem}
Suppose $\mat{X}$ is a \emph{normally} distributed $p$-vector with expectation $\mat{\mu} =\EXP\left[\mat{X}\right]$ and covariance matrix $\mat{\Sigma}=\COVMAT(\mat{X})$. Then $\mat{X}$ has a \emph{probability density function} given by
\begin{equation}\label{eq:multivarnormaldensity}
\mat{x} \mapsto \frac{1}{\sqrt{\det(\mat{\Sigma})} \left(2\pi\right)^{p/2}} \exp\left[ -\frac{1}{2} (\mat{x} - \mat{\mu})^* \mat{\Sigma}^{-1} (\mat{x} - \mat{\mu}) \right]
\end{equation}
\end{theorem}
\begin{remark}
The condition that $\mat{X}$ is normally distributed is necessary: if $\mat{X}$ is Gaussian, but not normal, it will take values in an \emph{affine subspace of $\mathbb{R}^p$} (such as a plane, as subspace of $\mathbb{R}^3$). If a density function were to exist, it would have support of zero measure, and integrating over $\mathbb{R}^p$ would yield $0$, instead of $1$.
\end{remark}
%\begin{theorem}
%The mode of a Gaussian distribution is its mean.\todo{(waar) moet dit?}
%\end{theorem}
\section{One linear condition}
\begin{definition}
Given a $\mat{r} \in \mathbb{R}^p$ and $b \in \mathbb{R}$, the set $A\subseteq \mathbb{R}^p$ of solutions to the equation
\[
\left\langle \mat{r}, \mat{x} \right\rangle = b
\]
is called a \emph{plane in $\mathbb{R}^p$}\index{plane}. The set $B \subseteq \mathbb{R}^p$ of solutions to
\[
\left\langle \mat{r}, \mat{x} \right\rangle \leq b
\]
is a \emph{half-space in $\mathbb{R}^p$}.
When $b=1$, we write $\mat{r}_A$ (or $\mat{r}_B$), and we say that $\mat{r}_A$ ($\mat{r}_B$) is a \define{pillar} for $A$ (or $B$). Two planes are called \emph{parallel} if they do not intersect.
\end{definition}
\begin{remark}
When $b \neq 1$, we can scale $\mat{r}$ to create a pillar for $A$ or $B$, unless $b=0$ (which is the case if and only if $\mat{0} \in A$).
\end{remark}
\begin{remark}
$A$ is the \emph{boundary} of $B$, \ie $A = \partial B$.
\end{remark}

Geometrically, a pillar can be interpreted as a vector from the origin that crosses the plane orthogonally. Its length is \emph{not} the distance between the origin and the plane, but rather the \emph{inverse} of this distance.
%
%\towrite{Orthogonal projection onto a plane}

\begin{definition}
A \emph{convex polyhedron in $\mathbb{R}^p$}\index{polyhedron} is the intersection of finitely many half-spaces in $\mathbb{R}^p$, and can be written as the set of solutions to
\[
\mat{R}\mat{x} \leq \mat{b},
\]
where each $i$th row of $\mat{R}$, together with $\mel{b}_i$, defines one of the intersecting half-spaces.
\end{definition}
The boundary of a convex polyhedron is contained in the union of planes corresponding to the half-spaces. This relation is strict, in general.

The convex polyhedra in $\mathbb{R}$ are exactly all (possibly infinite) closed intervals. Convex polyhedra in $\mathbb{R}^3$ are familiar shapes, like a cube or a pyramid, but they might also be unbounded, like a pyramid with infinitely deep foundations. 
Objects that are \emph{not} convex polyhedra are spheres (no finite intersection of half-spaces) and donuts (not convex, which as the name suggest, is a property of any convex polyhedron\footnote{Using the linearity of the inner product, one easily finds that half-spaces are convex, and so are their intersections.}).

\subsection{Feasibility region}
In this thesis, we model the \emph{power injection} of a grid as a normally distributed stochastic vector, where each coordinate corresponds to the amount of power injected at a node of the network. Positive values denote net generation (injection) and negative values are assigned to net consumption. The stochastic behaviour originates from \emph{renewable sources}: wind and solar, and their correlations arise from correlated weather. 

The transmission network was built to transfer power from one node to another, \ie have a non-zero power injection.
In Chapter \ref{chap:grid}, we will learn that not all power injections are \emph{feasible}, as some might cause one of the transmission lines to overload. The \emph{feasibility region} (the set of power injections that can be used) is, to some approximation,\footnote{We make this \emph{DC approximation} more precise in Chapter \ref{chap:model}.} a \emph{convex polyhedron}.

Using historical generation series, we can estimate the covariance of this power injection. Because the amount of current flowing through a line is a linear function of the power injection (by the same approximation), we can use Corollary \ref{cor:innerproductisgaussian} to determine the marginal distribution of line current, which gives the probability of an overload failure. A failure of this kind (\ie caused by a fluctuation of renewable injection) is called an \define{emergent failure}.

Current studies (notably \cite{Nesti2018emergentfailures} and \cite{Chertkov2011}) take this one step further, by determining \emph{the most probable power injection that caused the emergent failure to occur}. We follow the approach of \cite{Nesti2018emergentfailures}, which is generalised in the following Theorem. Here, the plane $A$ can be taken to be one of the \emph{boundary planes of the feasibility region, corresponding to one of the lines}, and the event $\mat{X} \in A$ is the \emph{event that this line overloads}.

\begin{theorem}\label{thm:modeofaplaneconditional}
Let $\mat{X} \, \sim \, \gaussdistr(\mat{\mu}, \mat{\Sigma})$ be a normally distributed $p$-vector with density function $f_{\mat{X}}$, and let $A\subseteq \mathbb{R}^p$ be a plane, given by a pillar $\mat{r}_A \in \mathbb{R}^p$. Then $\mat{X} \mid \mat{X} \in A$ is Gaussian distributed, and has mode
\begin{equation}
\tilde{\mat{x}}_{A} = \argmax_{\mat{x} \in A} f_{\mat{X}}(\mat{x}) = \mat{\mu}  + \frac{1 - \left\langle \mat{\mu}, \mat{r}_A \right\rangle}{\left\langle  \mat{\Sigma} \mat{r}_A, \mat{r}_A \right\rangle} \mat{\Sigma} \mat{r}_A.\label{eq:modeofaplaneconditional}
\end{equation}
\end{theorem}
\begin{proof}%\todo{inproduct rx is verkeerd om}
We will start with the special case of $\mat{\Sigma} = \mat{I}$, and work our way towards the general case.\footnote{This process uses the so-called the \emph{standardised}\index{distribution!standardised normal} form of $\mat{X}$.}

\emph{Step 1. The case of unit covariance.}\\
Suppose $\mat{X}$ is \emph{elementary} normally distributed, with all marginal variances equal to one, \ie $\mat{\Sigma} = \mat{\Sigma}^{-1} = \mat{I}$. The probability density function of $X$ (see Equation (\ref{eq:multivarnormaldensity})) then reduces to:
\[
\mat{x} \mapsto \frac{1}{\left(2\pi\right)^{p/2}} \exp\left[ \frac{-\norm{\mat{x} - \mat{\mu}}^2}{2}  \right].
\]
This is a \emph{decreasing} function of the \emph{distance between $\mat{x}$ and $\mat{\mu}$}, and so its mode is obtained when this distance is minimal.

Because $A$ is an affine subspace of $\mathbb{R}^p$, the distance between $\mat{x} \in A$ and $\mat{\mu}$ is minimal when $\mat{x}$ is the \emph{orthogonal projection} of $\mat{\mu}$ onto $A$, which is given by:
\[
\tilde{\mat{x}}_{A} = \mat{\mu} + \frac{1 - \left\langle \mat{\mu}, \mat{r}_A \right\rangle}{\left\langle \mat{r}_A, \mat{r}_A \right\rangle} \mat{r}_A.
\]

\emph{Step 2. The case of an arbitrary elementary normal distribution.}\\
Let us drop the assumption that all marginal variances are equal to one. There exist $\lambda_1, \dots, \lambda_p \in \mathbb{R}_{>0}$ such that $\mat{\Sigma} = \mat{\Lambda} = \diag(\lambda_1, \dots, \lambda_p)$, and $\mat{\Lambda}^{t} = \diag(\lambda_1^{t}, \dots, \lambda_p^{t})$ for any $t \in \mathbb{R}$.%\todo{Dit zijn de varianties, gebruik simgasquared?}
With $t=-\frac{1}{2}$, we find that $\mat{\Lambda}^{-\frac{1}{2}}\mat{X}$ is elementary normally distributed, with mean $\mat{\Lambda}^{-\frac{1}{2}}$ and unit covariance matrix (Theorem \ref{thm:linearmapofgaussian}).
Applying the same transformation $\mat{\Lambda}^{-\frac{1}{2}}$ to $A$ yields a new plane, which has pillar $\mat{\Lambda}^{\frac{1}{2}}\mat{r}_A$ (not $\mat{\Lambda}^{-\frac{1}{2}}\mat{r}_A$!):

For each $\mat{x} \in \mathbb{R}^p$, we have

$\mat{x} \in A \iff $
\begin{gather*}
\left\langle \mat{x}, \mat{r}_A \right\rangle = 1
\iff \sum_{i=1}^p \mel{x}_i \mel{r}_{A\,i} = 1
\iff \sum_{i=1}^p \lambda_{i}^{-\frac{1}{2}}\mel{x}_i \lambda_{i}^{\frac{1}{2}} \mel{r}_{A\,i} = 1 
\iff \\
\left\langle \mat{\Lambda}^{-\frac{1}{2}}\mat{x}, \mat{\Lambda}^{\frac{1}{2}}\mat{r}_A \right\rangle = 1
\iff
\left\langle \mat{\Lambda}^{-\frac{1}{2}}\mat{x}, \mat{r}_{\mat{\Lambda}^{-\frac{1}{2}}(A)} \right\rangle = 1
\end{gather*}

\hfill $\iff \mat{\Lambda}^{-\frac{1}{2}}\mat{x} \in \mat{\Lambda}^{-\frac{1}{2}}(A).$

This means that the plane $\mat{\Lambda}^{-\frac{1}{2}}(A)$ is defined by the pillar $\mat{\Lambda}^{\frac{1}{2}}\mat{r}_A$.

We can now apply our earlier result, and we find:

\begin{gather*}
\tilde{\mat{x}}_{A} = \mat{\Lambda}^{\frac{1}{2}} 
\oversortoftilde{\left(
\mat{\Lambda}^{-\frac{1}{2}} \mat{x}\right)_{\mat{\Lambda}^{-\frac{1}{2}}(A)}
}
=
\mat{\Lambda}^{\frac{1}{2}} 
\left(
\mat{\Lambda}^{-\frac{1}{2}} \mat{\mu}  + \frac{1 - \left\langle \mat{\Lambda}^{-\frac{1}{2}} \mat{\mu}, \mat{\Lambda}^{\frac{1}{2}} \mat{r}_A \right\rangle}{\left\langle \mat{\Lambda}^{\frac{1}{2}} \mat{r}_A, \mat{\Lambda}^{\frac{1}{2}} \mat{r}_A \right\rangle} \mat{\Lambda}^{\frac{1}{2}} \mat{r}_A
\right) \\
=
\mat{\mu}  + \frac{1 - \left\langle \mat{\Lambda}^{-\frac{1}{2}} \mat{\mu}, \mat{\Lambda}^{\frac{1}{2}} \mat{r}_A \right\rangle}{\left\langle \mat{\Lambda}^{\frac{1}{2}} \mat{r}_A, \mat{\Lambda}^{\frac{1}{2}} \mat{r}_A \right\rangle} \mat{\Lambda} \mat{r}_A
=
\mat{\mu}  + \frac{1 - \left\langle \mat{\mu}, \mat{r}_A \right\rangle}{\left\langle \mat{\Lambda}  \mat{r}_A,\mat{r}_A \right\rangle} \mat{\Lambda} \mat{r}_A.
\end{gather*}

\emph{Step 3. The general case.}\\
Finally, we consider the general case where $\mat{X}$ is normally distributed. If so, there exists orthogonal $\mat{Q}$ such that $\mat{\Sigma}=\mat{Q}\mat{\Lambda}\mat{Q}^{-1}$, with $\mat{\Lambda}$ a diagonal matrix. In other words, the orthogonal map $\mat{Q}^{-1}$ maps $\mat{X}$ into an \emph{elementary} normally distributed vector.\footnote{$\mat{Q}^{-1}\mat{X}$ is elementary normally distributed, but it does not necessarily have a unit covariance matrix.}
Applying the same transformation $\mat{Q}^{-1}$ to $A$ yields a new plane, which is defined by the pillar $\mat{Q}^{-1}\mat{r}_A$. This derivation is more straightforward, since $\mat{Q}^{-1}$ is orthogonal:

For each $\mat{x} \in \mathbb{R}^p$, we have

$\mat{x} \in A \iff $
\begin{gather*}
\left\langle \mat{x}, \mat{r}_A \right\rangle = 1
\iff \left\langle \mat{Q}^{-1} \mat{x}, \mat{Q}^{-1} \mat{r}_A \right\rangle = 1
\iff \left\langle \mat{Q}^{-1} \mat{x}, \mat{r}_{\mat{Q}^{-1}(A)} \right\rangle = 1
\end{gather*}

\hfill $\iff \mat{Q}^{-1}\mat{x} \in\mat{Q}^{-1}(A).$

Using our previous result, we find:

\begin{gather*}
\tilde{\mat{x}}_{A}
=
\mat{Q}
\oversortoftilde{\left(
\mat{Q}^{-1}\mat{x}\right)_{\mat{Q}^{-1}(A)}
}
=
\mat{Q}
\left(
\mat{Q}^{-1}\mat{\mu}  + \frac{1 - \left\langle \mat{Q}^{-1} \mat{\mu}, \mat{Q}^{-1}\mat{r}_A \right\rangle}{\left\langle \mat{\Lambda} \mat{Q}^{-1} \mat{r}_A,\mat{Q}^{-1} \mat{r}_A \right\rangle} \mat{\Lambda} \mat{Q}^{-1} \mat{r}_A
\right) \\
=
\mat{\mu}  + \frac{1 - \left\langle \mat{Q}^{-1} \mat{\mu}, \mat{Q}^{-1}\mat{r}_A \right\rangle}{\left\langle \mat{\Lambda} \mat{Q}^{-1} \mat{r}_A,\mat{Q}^{-1} \mat{r}_A \right\rangle} \mat{Q} \mat{\Lambda} \mat{Q}^{-1} \mat{r}_A
=
\mat{\mu}  + \frac{1 - \left\langle \mat{\mu}, \mat{r}_A \right\rangle}{\left\langle  \mat{Q} \mat{\Lambda} \mat{Q}^{-1} \mat{r}_A, \mat{r}_A \right\rangle} \mat{Q} \mat{\Lambda} \mat{Q}^{-1} \mat{r}_A \\
=
\mat{\mu}  + \frac{1 - \left\langle \mat{\mu}, \mat{r}_A \right\rangle}{\left\langle  \mat{\Sigma} \mat{r}_A, \mat{r}_A \right\rangle} \mat{\Sigma} \mat{r}_A.
\end{gather*}

\end{proof}

When we replace the \emph{plane} $A$ by a \emph{half-space} $B$ in Theorem \ref{thm:modeofaplaneconditional}, there are two cases to consider. If $\mat{\mu} \in B$, then $\tilde{\mat{x}}_{B} = \mat{\mu}$, because the mode of $\mat{X}$ is $\mat{\mu}$, which is contained in $B$. 

In the non-trivial case of $\mat{\mu} \notin B$, we find that $\tilde{\mat{x}}_{B} = \tilde{\mat{x}}_{A}$, where $A \defeq \partial B$ is the \emph{boundary} of $B$, which is a plane, given by the same pillar: $\mat{r}_A = \mat{r}_B$. A geometrical argument is as follows: by reducing to the case of an elementary normally distributed $\mat{X}$ with a unit covariance matrix, the mode is the point of $B$ that minimises the distance to $\mat{\mu}$. Since $\mat{\mu} \notin B$, this point must lie on the boundary of $B$, \ie $\tilde{\mat{x}}_{B} \in A$. Because $A$ is a subset of $B$, we can proceed to compute $\tilde{\mat{x}}_{A}$, which is then necessarily the mode of $\mat{X} \, \mid \, B$.

For a half-plane $B$, we now have an explicit expression for $\tilde{\mat{x}}_{B}$, \ie the \emph{most likely value} of $\mat{X}$, given that $\mat{X} \in B$. What can we say about the \emph{expected value} of $\mat{X} \, \mid \, B$? Geometrically, one can imagine that $\EXP [\mat{X} \, \mid \, \mat{X} \in B]$ lies close to $\tilde{\mat{x}}_{B}$, but `shifted inwards', away from $\partial B$ (in the direction of its pillar). We have not found a direct method to compute this expected value. \cite{Nesti2018emergentfailures} introduce a scaling factor $\epsilon>0$ to the covariance of $\mat{X}$ (\ie $\mat{X}_{\epsilon} \, \sim \, \gaussdistr(\mat{\mu}, \epsilon\mat{\Sigma})$), and show that the expected value \emph{converges} to $\tilde{\mat{x}}_{B}$, the mode,\footnote{The factor $\epsilon$ disappears in (\ref{eq:modeofaplaneconditional}).} as $\epsilon \rightarrow 0$, using the tools of Large Deviations Theory.

One can interpret this result as saying that the probability density of $\mat{X}_{\epsilon} | B$ gets \emph{concentrated close to the boundary of $B$}. In the one-dimensional case, this statement\footnote{\ie the \emph{tail distribution} of $X \, \sim \, \gaussdistr(0,\epsilon^2)$, given $X \geq 1$, becomes \emph{steeper} as $\epsilon \rightarrow 0$.} is easy to demonstrate, see \eg \cite{Touchette2009}.
%
%\todo[inline]{Is de marginale verdeling gaussiaans verdeeld? (Ze is niet normaal verdeeld.) Wat is de covariantiematrix? Je zou als Stap 0 kunnen transformeren naar een $\mathbf{r}_A$ die parallel aan een van de assen loopt, en dan dat coordinaat gelijk aan $\norm{\mathbf{r}_A}$ stellen? Maar dan klopt de verwachtingswaarde niet meer?}
%
%\todo[inline]{Het zou mooier zijn om dit resultaat toe te passen op een algemene Gauss-verdeling, die niet per se normaal is. De marginele verdeling bestaat dan alleen niet altijd, namelijk wanneer $A \cap \Ima(\mathbf{X}) = \emptyset$. Als ze wel bestaat, dan gok ik dat het Gaussiaans is.
%
%Het is nu niet toe te passen op line flows (want die zijn normaal, wel Gaussiaans), maar wel op de injectie. Wanneer je dan de meest likely injectie hebt, kan je afleiden wat de flow is. Dit komt overeen met elkaar?
%
%De covariantiematrix van de flow is ook bijna singulair (numeriek gezien singulair), omdat de correlaties zo sterk zijn. Maar ik denk dat je kan beargumenteren dat de kans op singulaire covariantiematrix $0$ is, en dan kan de stelling dus wel worden toegepast. Numeriek is de bijna-singulariteit ook geen probleem (denk ik), omdat in de uiteindelijke uitdrukking de inverse van sigma niet voorkomt.}
\end{document}